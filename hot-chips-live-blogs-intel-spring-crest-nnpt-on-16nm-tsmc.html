<!doctype html><html lang=en><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=referrer content="no-referrer"><meta name=description content="08:23PM EDT - Intel is showing us some of the design features of its new ML training product, Spring Crest. 08:23PM EDT - NNP-T = Training 08:24PM EDT - Spring Crest is what Intel purchased when it acquired Nervana in 2016. THis is the big chip that came with the acquisition"><meta name=robots content="index,follow,noarchive"><link href="https://fonts.googleapis.com/css?family=Open+Sans:400|Old+Standard+TT:400&display=swap" rel=stylesheet media=print type=text/css onload='this.media="all"'><title>Intel Spring Crest NNP-T on 16nm TSMC</title><link rel=canonical href=./hot-chips-live-blogs-intel-spring-crest-nnpt-on-16nm-tsmc.html><style>*{border:0;font:inherit;font-size:100%;vertical-align:baseline;margin:0;padding:0;color:#000;text-decoration-skip:ink}body{font-family:open sans,myriad pro,Myriad,sans-serif;font-size:17px;line-height:160%;color:#1d1313;max-width:700px;margin:auto}p{margin:20px 0}a img{border:none}img{margin:10px auto;max-width:100%;display:block}.left-justify{float:left}.right-justify{float:right}pre,code{font:12px Consolas,liberation mono,Menlo,Courier,monospace;background-color:#f7f7f7}code{font-size:12px;padding:4px}pre{margin-top:0;margin-bottom:16px;word-wrap:normal;padding:16px;overflow:auto;font-size:85%;line-height:1.45}pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:0 0;border:0}pre code{display:inline;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}pre code::before,pre code::after{content:normal}em,q,em,dfn{font-style:italic}.sans,html .gist .gist-file .gist-meta{font-family:open sans,myriad pro,Myriad,sans-serif}.mono,pre,code,tt,p code,li code{font-family:Menlo,Monaco,andale mono,lucida console,courier new,monospace}.heading,.serif,h1,h2,h3{font-family:old standard tt,serif}strong{font-weight:600}q:before{content:"\201C"}q:after{content:"\201D"}del,s{text-decoration:line-through}blockquote{font-family:old standard tt,serif;text-align:center;padding:50px}blockquote p{display:inline-block;font-style:italic}blockquote:before,blockquote:after{font-family:old standard tt,serif;content:'\201C';font-size:35px;color:#403c3b}blockquote:after{content:'\201D'}hr{width:40%;height:1px;background:#403c3b;margin:25px auto}h1{font-size:35px}h2{font-size:28px}h3{font-size:22px;margin-top:18px}h1 a,h2 a,h3 a{text-decoration:none}h1,h2{margin-top:28px}#sub-header,.date{color:#403c3b;font-size:13px}#sub-header{margin:0 4px}#nav h1 a{font-size:35px;color:#1d1313;line-height:120%}.posts_listing a,#nav a{text-decoration:none}li{margin-left:20px}ul li{margin-left:5px}ul li{list-style-type:none}ul li:before{content:"\00BB \0020"}#nav ul li:before,.posts_listing li:before{content:'';margin-right:0}#content{text-align:left;width:100%;font-size:15px;padding:60px 0 80px}#content h1,#content h2{margin-bottom:5px}#content h2{font-size:25px}#content .entry-content{margin-top:15px}#content .date{margin-left:3px}#content h1{font-size:30px}.highlight{margin:10px 0}.posts_listing{margin:0 0 50px}.posts_listing li{margin:0 0 25px 15px}.posts_listing li a:hover,#nav a:hover{text-decoration:underline}#nav{text-align:center;position:static;margin-top:60px}#nav ul{display:table;margin:8px auto 0}#nav li{list-style-type:none;display:table-cell;font-size:15px;padding:0 20px}#links{display:flex;justify-content:space-between;margin:50px 0 0}#links :nth-child(1){margin-right:.5em}#links :nth-child(2){margin-left:.5em}#not-found{text-align:center}#not-found a{font-family:old standard tt,serif;font-size:200px;text-decoration:none;display:inline-block;padding-top:225px}@media(max-width:750px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:28px}#nav li{font-size:13px;padding:0 15px}#content{margin-top:0;padding-top:50px;font-size:14px}#content h1{font-size:25px}#content h2{font-size:22px}.posts_listing li div{font-size:12px}}@media(max-width:400px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:22px}#nav li{font-size:12px;padding:0 10px}#content{margin-top:0;padding-top:20px;font-size:12px}#content h1{font-size:20px}#content h2{font-size:18px}.posts_listing li div{font-size:12px}}@media(prefers-color-scheme:dark){*,#nav h1 a{color:#fdfdfd}body{background:#121212}pre,code{background-color:#262626}#sub-header,.date{color:#bababa}hr{background:#ebebeb}}</style></head><body><section id=nav><h1><a href=./index.html>RiffVib</a></h1><ul><li><a href=./index.xml>Rss</a></li><li><a href=./sitemap.xml>Sitemap</a></li></ul></section><section id=content><h1>Intel Spring Crest NNP-T on 16nm TSMC</h1><div id=sub-header>January 2024 Â· 4 minute read</div><div class=entry-content><p><a id=post0819202309 href=#><span class=lb_time>08:23PM EDT</span></a> - Intel is showing us some of the design features of its new ML training product, Spring Crest.</p><p><a id=post0819202353 href=#><span class=lb_time>08:23PM EDT</span></a> - NNP-T = Training</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14757/IMG_20190819_172340_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819202418 href=#><span class=lb_time>08:24PM EDT</span></a> - Spring Crest is what Intel purchased when it acquired Nervana in 2016. THis is the big chip that came with the acquisition</p><p><a id=post0819202443 href=#><span class=lb_time>08:24PM EDT</span></a> - Trend in Neural Networks means compute requirements doubles every 3.5x months</p><p><a id=post0819202529 href=#><span class=lb_time>08:25PM EDT</span></a> - Need to fill the die with as much compute that can be fed</p><p><a id=post0819202545 href=#><span class=lb_time>08:25PM EDT</span></a> - DL is as much as a communication problem as it is a compute problem</p><p><a id=post0819202553 href=#><span class=lb_time>08:25PM EDT</span></a> - Need a scale-out model for larger models</p><p><a id=post0819202628 href=#><span class=lb_time>08:26PM EDT</span></a> - Want to train a model as fast as possible within a power budget. Aim for high utilization, and a scalable solution</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14757/IMG_20190819_172558_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819202643 href=#><span class=lb_time>08:26PM EDT</span></a> - Balance between compute, comms, and memory</p><p><a id=post0819202655 href=#><span class=lb_time>08:26PM EDT</span></a> - Best is to be compute bound on all but the smallest problems</p><p><a id=post0819202705 href=#><span class=lb_time>08:27PM EDT</span></a> - Keep data local and reuse it as much as possible</p><p><a id=post0819202714 href=#><span class=lb_time>08:27PM EDT</span></a> - Consistent programming model</p><p><a id=post0819202730 href=#><span class=lb_time>08:27PM EDT</span></a> - Flexibility for future workloads</p><p><a id=post0819202751 href=#><span class=lb_time>08:27PM EDT</span></a> - Spring Crest uses 2.5D</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14757/IMG_20190819_172739_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819202757 href=#><span class=lb_time>08:27PM EDT</span></a> - PCIe Gen 4 x16 with host CPU</p><p><a id=post0819202803 href=#><span class=lb_time>08:28PM EDT</span></a> - 4x 8GB HBM2</p><p><a id=post0819202815 href=#><span class=lb_time>08:28PM EDT</span></a> - 24 Tensor Processors (TPCs), Up to 119 TOPs</p><p><a id=post0819202825 href=#><span class=lb_time>08:28PM EDT</span></a> - 8x8 lanes SerDes for chip-to-chip communications</p><p><a id=post0819202848 href=#><span class=lb_time>08:28PM EDT</span></a> - Built on 16FF+ TSMC with CoWoS</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14757/IMG_20190819_172830_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819202904 href=#><span class=lb_time>08:29PM EDT</span></a> - 680mm2 with 1200mm2 passive interposer, 27 billion transistors</p><p><a id=post0819202909 href=#><span class=lb_time>08:29PM EDT</span></a> - Up to 1.1 GHz Core frequ</p><p><a id=post0819202913 href=#><span class=lb_time>08:29PM EDT</span></a> - HBM2-2400</p><p><a id=post0819202923 href=#><span class=lb_time>08:29PM EDT</span></a> - Supports PCIe and OAM (Open Compute)</p><p><a id=post0819202939 href=#><span class=lb_time>08:29PM EDT</span></a> - TensorFlow and PaddlePaddle first frameworks supported, more to come. Uses NGraph</p><p><a id=post0819203007 href=#><span class=lb_time>08:30PM EDT</span></a> - Intel provide the low level compiler performance optimizations</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14757/IMG_20190819_172943_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819203025 href=#><span class=lb_time>08:30PM EDT</span></a> - Tensor based ISA</p><p><a id=post0819203030 href=#><span class=lb_time>08:30PM EDT</span></a> - Limited instruction set</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14757/IMG_20190819_173015_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819203041 href=#><span class=lb_time>08:30PM EDT</span></a> - Extensible with custom microcontroller custom instructions</p><p><a id=post0819203053 href=#><span class=lb_time>08:30PM EDT</span></a> - Same distributed model on-chip and off-chip</p><p><a id=post0819203058 href=#><span class=lb_time>08:30PM EDT</span></a> - Compute has affinity for local data</p><p><a id=post0819203108 href=#><span class=lb_time>08:31PM EDT</span></a> - DL worklaods are dominated by a limited set of operations</p><p><a id=post0819203116 href=#><span class=lb_time>08:31PM EDT</span></a> - Explicit SW memory management and message passing</p><p><a id=post0819203155 href=#><span class=lb_time>08:31PM EDT</span></a> - 150-250W power</p><p><a id=post0819203225 href=#><span class=lb_time>08:32PM EDT</span></a> - Here's a TPC</p><p><a id=post0819203236 href=#><span class=lb_time>08:32PM EDT</span></a> - On-chip router, controller, two arrays, memory</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14757/IMG_20190819_173216_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819203314 href=#><span class=lb_time>08:33PM EDT</span></a> - Each 32x32 array has pre-op and post-op support</p><p><a id=post0819203323 href=#><span class=lb_time>08:33PM EDT</span></a> - dedicated convolution engine for non-MAC compute</p><p><a id=post0819203352 href=#><span class=lb_time>08:33PM EDT</span></a> - BFloat16 support with FP32 accumulation</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14757/IMG_20190819_173337_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819203444 href=#><span class=lb_time>08:34PM EDT</span></a> - BF16 32x32 MAC Core</p><p><a id=post0819203457 href=#><span class=lb_time>08:34PM EDT</span></a> - 2x Multiply Cores per TPC to amortize SoC resources</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14757/IMG_20190819_173428_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819203517 href=#><span class=lb_time>08:35PM EDT</span></a> - Compound vector pipeline with DL specific optimizations on non-GEMM ops</p><p><a id=post0819203559 href=#><span class=lb_time>08:35PM EDT</span></a> - 1.22 TBps raw HBM2 bandwidth</p><p><a id=post0819203608 href=#><span class=lb_time>08:36PM EDT</span></a> - 2.5MB / TPC local scratchpad memory</p><p><a id=post0819203617 href=#><span class=lb_time>08:36PM EDT</span></a> - Native Tensor Transpose without any overhead</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14757/IMG_20190819_173539_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819203627 href=#><span class=lb_time>08:36PM EDT</span></a> - 1.4 TBps local read/write bw per TPC</p><p><a id=post0819203639 href=#><span class=lb_time>08:36PM EDT</span></a> - Cna do TPC-to-TPC data movement without HBM involvement</p><p><a id=post0819203709 href=#><span class=lb_time>08:37PM EDT</span></a> - 2D Meshes, multiple meshes for different data types</p><p><a id=post0819203719 href=#><span class=lb_time>08:37PM EDT</span></a> - prioritized for throughput over latency</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14757/IMG_20190819_173651_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819203736 href=#><span class=lb_time>08:37PM EDT</span></a> - 1.3 TBps bandwidth in each direction</p><p><a id=post0819203810 href=#><span class=lb_time>08:38PM EDT</span></a> - Designed for a fully connected topology</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14757/IMG_20190819_173751_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819203821 href=#><span class=lb_time>08:38PM EDT</span></a> - Looks like one large system to simplify the software model</p><p><a id=post0819203831 href=#><span class=lb_time>08:38PM EDT</span></a> - Up to 1024 nodes supported gluelessly</p><p><a id=post0819203847 href=#><span class=lb_time>08:38PM EDT</span></a> - 3.58 TBps total bidirectional SerDes BW per chip</p><p><a id=post0819203903 href=#><span class=lb_time>08:39PM EDT</span></a> - Fully programable router with multi-cast support and virtual channel support</p><p><a id=post0819203959 href=#><span class=lb_time>08:39PM EDT</span></a> - Aiming for high utilization across many GEMM sizes</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14757/IMG_20190819_173939_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819204017 href=#><span class=lb_time>08:40PM EDT</span></a> - Most architectures do well on large square GEMMs. Not all hardware can do different matrix sizes well</p><p><a id=post0819204047 href=#><span class=lb_time>08:40PM EDT</span></a> - Looking at GEMM utilization that is difficult to solve</p><p><a id=post0819204201 href=#><span class=lb_time>08:42PM EDT</span></a> - Ring topology bandwidth benchmarked across 32-chips</p><p><a id=post0819204219 href=#><span class=lb_time>08:42PM EDT</span></a> - Equivalent bandwidth between cards and between racks</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14757/IMG_20190819_174139_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819204327 href=#><span class=lb_time>08:43PM EDT</span></a> - Performance measured using 22 TPCs at 900 MHz core clock and 2 GHz HBM. Host is Xeon Gold 6130T @ 2.1 GHz</p><p><a id=post0819204354 href=#><span class=lb_time>08:43PM EDT</span></a> - Whisper connectivity</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14757/IMG_20190819_174434_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819204518 href=#><span class=lb_time>08:45PM EDT</span></a> - Latency card-to-card at 3-9 microseconds, cross chassis at 30-36 microseconds</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14757/IMG_20190819_174541_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819204601 href=#><span class=lb_time>08:46PM EDT</span></a> - Coming to customers soon</p><p><a id=post0819204604 href=#><span class=lb_time>08:46PM EDT</span></a> - Q&A</p><p><a id=post0819204704 href=#><span class=lb_time>08:47PM EDT</span></a> - Q: How do you support structured sparsity? A: More benchmarks to come</p><p><a id=post0819204731 href=#><span class=lb_time>08:47PM EDT</span></a> - Q: MLperf? A: Can't comment. More data before the end of the year</p><p><a id=post0819204929 href=#><span class=lb_time>08:49PM EDT</span></a> - That's a wrap. Next up Cerebras</p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmivp6x7orrAp5utnZOde6S7zGiqoaenZH51g5RwZqGnpGKwqbXPrGSloaaaeqO4zqCqZqGeqbKtedKpqaKml2Kws7HSrWSnpqCperC6jGptp6VdqcCurw%3D%3D</p></div><div id=links><a href=./jim-cramer.html>&#171;&nbsp;Jim Cramer - Bio, Age, Net Worth, Height, Nationality, Facts</a>
<a href=./florence-pugh-black-see-through-gown-tie-front-bodice.html>Florence Pugh Wears a Black See-Through Gown With a Tie-Front Bodice&nbsp;&#187;</a></div></section><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/banner.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>